{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain_openai\n",
        "!pip install -U langchain-anthropic\n",
        "!pip install json_repair"
      ],
      "metadata": {
        "id": "Yj5EEe47JV-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Planner: A Proof of Concept for Automated Prompt Generation\n",
        "\n",
        "The Prompt Planner is a proof of concept designed to demonstrate how large language models (LLMs) can be used to automatically generate effective prompts.\n",
        "\n",
        "# It operates through a two-step process:\n",
        "\n",
        "### Step 1) Information Collection:\n",
        "The system begins by acting as an information collector, using LLMs to engage with users and gather necessary details based on a brief task description. This interaction is designed to build a comprehensive understanding of the task at hand.\n",
        "\n",
        "### Step 2) Prompt Planning:\n",
        "Once the detailed task description is established from the userâ€™s inputs, this information is fed into the prompt planner. The planner then constructs a tailored prompt, optimized to instruct another LLM to produce outputs in specific formats like JSON or markdown, based on the detailed task description.\n",
        "\n",
        "\n",
        "This agentic approach leverages multiple LLMs that pass information to one another, refining the process of prompt engineering to meet user expectations efficiently. The dynamically changing inputs and the ability to optimize output formats make the Prompt Planner a decent starting point for people how doesnt know how to create an effective prompts based on given inputs and desired output requirements.\n"
      ],
      "metadata": {
        "id": "eGVY0_ghGpjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ntq3rnMD5aYD"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "import json_repair\n",
        "from langchain_anthropic import AnthropicLLM\n",
        "import os\n",
        "from langchain.prompts import SystemMessagePromptTemplate,HumanMessagePromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser,StrOutputParser\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup env keys for LLM, Currently only supports \"openai\" and \"anthropic\" models"
      ],
      "metadata": {
        "id": "BYnHv_hpJaek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Azure Open ai envs\n",
        "AZURE_ENDPOINT= \"\"\n",
        "AZURE_API_VERSION=\"\"\n",
        "AZURE_OPENAI_API_KEY= \"\"\n",
        "AZURE_LLM_MODEL=''\n",
        "\n",
        "## Anthropic envs\n",
        "os.environ['ANTHROPIC_API_KEY']=\"\"\n"
      ],
      "metadata": {
        "id": "7B2AKMiB9oEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils funtions"
      ],
      "metadata": {
        "id": "Qg8aNyTk-EsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to select and configure a language model (LLM) based on a given choice.\n",
        "def get_llm(llm_choice):\n",
        "    # Check if the chosen LLM is 'Anthropic'.\n",
        "    if llm_choice == 'azure_openai':\n",
        "        # Create an instance of AzureChatOpenAI configured for the Anthropic model.\n",
        "        # This setup involves specifying various parameters like API keys and endpoint details specific to Azure.\n",
        "        llm = AzureChatOpenAI(azure_endpoint=AZURE_ENDPOINT,\n",
        "                              openai_api_version=AZURE_API_VERSION,\n",
        "                              api_key=AZURE_OPENAI_API_KEY,\n",
        "                              azure_deployment=AZURE_LLM_MODEL,\n",
        "                              temperature=0, max_tokens=2000)\n",
        "    # Check if the chosen LLM is 'openai'.\n",
        "    elif llm_choice == 'Anthropic':\n",
        "        # Create an instance of AnthropicLLM, specifically using the 'claude-2.1' model.\n",
        "        llm = AnthropicLLM(model='claude-2.1')\n",
        "    else:\n",
        "        # If an unrecognized LLM choice is provided, raise an error.\n",
        "        raise ValueError(\"Invalid LLM choice provided.\")\n",
        "\n",
        "    # Return the configured language model instance.\n",
        "    return llm\n",
        "\n",
        "def parse_and_save_html(content, file_name):\n",
        "    \"\"\"\n",
        "    Parses HTML content to extract information from <section> tags and saves it to a file.\n",
        "\n",
        "    Parameters:\n",
        "    - content: A string containing the HTML content.\n",
        "    - save_path: The file path where the extracted information will be stored.\n",
        "\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(content, 'html.parser')\n",
        "    file_name=file_name+'.txt'\n",
        "    #save_path=os.path.join(save_path, file_name)\n",
        "    with open(file_name, \"w\") as file:\n",
        "        sections = soup.find_all('section')\n",
        "        for section in sections:\n",
        "            # Use get to safely extract the class attribute, defaulting to an empty list\n",
        "            class_name = section.get('class', [None])[0]\n",
        "\n",
        "            # Extract the heading within each section\n",
        "            heading = section.find(['h1', 'h2'])\n",
        "            if heading:\n",
        "                file.write(f\"### {heading.text}\\n\")  # Write heading text with ### prefix\n",
        "\n",
        "            # Write the content of the section\n",
        "            for content in section.contents:\n",
        "                if content.name not in ['h1', 'h2']:  # Avoid duplicating the heading\n",
        "                    if content.name == 'ol' or content.name == 'ul':\n",
        "                        # Handle each list item separately\n",
        "                        list_items = content.find_all('li')\n",
        "                        for item in list_items:\n",
        "                            file.write(f\"{item.get_text(strip=True)}\\n\")\n",
        "                    else:\n",
        "                        # Write other types of content\n",
        "                        text = content.get_text(strip=True)\n",
        "                        if 'output-format' == class_name:  # Check if the class is 'output-format'\n",
        "                            text = text.replace('{', '{{')\n",
        "                            text = text.replace('}', '}}')\n",
        "\n",
        "                        if text:\n",
        "                            file.write(f\"{text}\\n\")\n",
        "            file.write(\"\\n\")  # Add a newline for better separation between sections\n",
        "\n",
        "    # Open the file in read mode\n",
        "    with open(file_name, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "\n",
        "    print(f\"#### Generated prompt is saved at {os.path.join(os.getcwd(), file_name)}\")\n",
        "    return content\n",
        "\n",
        "def interact_and_generate_description(task_requ_n_desc_chain, brief_task_desc):\n",
        "    \"\"\"\n",
        "    Interacts with the user by asking a series of questions, collects responses, and generates a task description.\n",
        "\n",
        "    Parameters:\n",
        "    - task_requ_n_desc_chain: a callable that invokes the backend model to generate questions and task descriptions.\n",
        "    - initial_data: the initial input data used to start the interaction (likely the initial task description).\n",
        "\n",
        "    Returns:\n",
        "    - The final task description generated based on user responses.\n",
        "    \"\"\"\n",
        "    chat_history = []\n",
        "    user_interaction = True\n",
        "    gen_ques_n_desc = task_requ_n_desc_chain.invoke({'desc': brief_task_desc, 'chat_history': chat_history})\n",
        "\n",
        "    while user_interaction:\n",
        "        # Parse the response from the chain\n",
        "        response_data = json_repair.loads(gen_ques_n_desc)\n",
        "        response_type = response_data['response_type']\n",
        "        response = response_data['response']\n",
        "\n",
        "        if response_type == 'questions':\n",
        "            for idx, ques in enumerate(response):\n",
        "                print(f\"Question {idx + 1}: {ques}\")\n",
        "                time.sleep(2)\n",
        "                user_response = input(\"Your answer: \")\n",
        "\n",
        "                # Store each interaction in chat history\n",
        "                chat = {'AI Message': ques, 'Human Message': user_response}\n",
        "                chat_history.append(chat)\n",
        "\n",
        "            # Notify that all questions have been answered, proceed to the next step\n",
        "            print('#########################')\n",
        "            print('\\n\\n')\n",
        "\n",
        "            print('##  Got the user response now procedding to generate the task description so that prompt planner can plan the excecution of task##')\n",
        "\n",
        "            print('\\n\\n')\n",
        "            print('#########################')\n",
        "\n",
        "            chat_history.append({'message': 'All questions answered, proceeding to generate the task description.'})\n",
        "            gen_ques_n_desc = task_requ_n_desc_chain.invoke({'desc': 'create an answer based on given background context', 'chat_history': chat_history})\n",
        "\n",
        "        elif response_type == 'task_description':\n",
        "            user_interaction = False\n",
        "            return response  # Return the generated task description\n",
        "\n",
        "\n",
        "def runnable(llm, sys_temp, human_temp=None):\n",
        "    # Convert the system template string to a SystemMessagePromptTemplate object.\n",
        "    sys_temp = SystemMessagePromptTemplate.from_template(sys_temp)\n",
        "\n",
        "    # Start with the system template in the prompts list.\n",
        "    prompts_list = [sys_temp]\n",
        "\n",
        "    # If a human template is provided, convert it to a SystemMessagePromptTemplate and append to the list.\n",
        "    if human_temp:\n",
        "        human_temp = SystemMessagePromptTemplate.from_template(human_temp)\n",
        "        prompts_list.append(human_temp)\n",
        "\n",
        "    # Combine all message templates into a single ChatPromptTemplate object.\n",
        "    prompt = ChatPromptTemplate.from_messages(prompts_list)\n",
        "\n",
        "    # Initialize the output parser to handle the structure of the response.\n",
        "    output_parser = StrOutputParser()\n",
        "\n",
        "    # Create a pipeline by chaining the prompt, language model (llm), and output parser together.\n",
        "    # This creates a flow where the prompt is processed by the llm, and its output is parsed by output_parser.\n",
        "    run = prompt | llm | output_parser\n",
        "\n",
        "    # Return the runnable pipeline that can be executed to get the processed output.\n",
        "    return run\n"
      ],
      "metadata": {
        "id": "_RID9xIq98cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts:"
      ],
      "metadata": {
        "id": "bILAq5J0-Yrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info_gather_temp_sys=\"\"\"\n",
        "Your role is to work as a task description creator, giving a brief description of the task by the user and formulating a detailed description of the task which specifies what is the task.  The task description prepared by you will be given to the Task planner who will create a plan on the execution of the task.\n",
        "\n",
        "Follow these two steps\n",
        "\n",
        "Step 1) Ask Questions:\n",
        "    a) Analyze the initial task description to identify any gaps in information. Formulate set of 5 questions to gather more context on the task's objective, the targeted audience, and typical scenarios where the task would be performed. The aim here is to ensure that the final task execution meets the user's expectations and satisfies their needs.\n",
        "    b) Use your domain knowledge to determine what additional context is needed to ensure the task outcome aligns with the user's expectations. Consider different approaches to the task and identify areas where the user's preferences or decisions could influence execution and generate a set of questions accordingly.\n",
        "\n",
        "Guidelines for Crafting Questions:\n",
        "    1. Ensure each question is clear and direct to encourage precise answers without requiring lengthy responses.\n",
        "    2. Aim for open-ended questions that elicit detailed answers but can be answered briefly and efficiently.\n",
        "    3. Focus on one aspect per question to avoid overwhelming the user and keep responses focused.\n",
        "\n",
        "Step 2) Formulate the Task Description:\n",
        "    a) Once get the answers to set of questions in chat history, DO NOT generete further question.\n",
        "    b) compile the information into a clear task description based on initial task description and user responses regarding their preferences and expectations.\n",
        "\n",
        "####### Here is the chat history containing the questions and their responses ##########\n",
        "chat_history:{chat_history}\n",
        "\n",
        "###### Output Format:\n",
        "    Your  output format is 'ONLY' JSON with two key responses:\n",
        "    {{'response_type':\"questions\" or \"task_description\", 'response': \"the list of question when the response_type is qquestions\" or \"the actuall task description one you get the answers of the question from the user\"}}\n",
        "\n",
        "Note:\n",
        "    - Limit yourself to a maximum of five questions, Be concise and direct to avoid producing questions that overwhelm the users in responding.\n",
        "    - Once you get the answers of the question you created from the chat history, formulate the task description , mask the resposne_type key to task description.\n",
        "    - Output Only JSON with any text before or after\n",
        "\"\"\"\n",
        "info_gather_temp_human=\"\"\"Heres is the brief description of the task: {desc}. Once you get the answers of step 1 questions then only proceed to step 2 and prepare the task_descriptions\"\"\"\n",
        "\n",
        "prompt_gen_sys = \"\"\"\n",
        "Your role is to create a task planning prompt based on a given task description, input variables, and desired output.\n",
        "The planning prompt contains step-by-step instructions on how the particular task should be executed to achieve the desired outcome.\n",
        "Here the input variables are the inputs using which a task will be executed, and the desired outcome specifies how the outcome of the task should be.\n",
        "\n",
        "Please follow the steps below to write an effective instruction prompt:\n",
        "\n",
        "a) Thorough Understanding: Carefully read the task description to fully understand what specific task needs to be done.\n",
        "b) Think and Planning: Utilize your knowledge to think about how the task needs to be done and then plan a set of step-by-step guidelines required to carry out the task effectively and accurately.\n",
        "c) Guideline Review: Carefully review each guideline to ensure they logically provide clear guidance. Anyone should be able to execute the task without any prior knowledge about it.\n",
        "d) Specify Output Format: Clearly mention how to construct the output format to ensure that the outcome of the task is presented as desired.\n",
        "\n",
        "Your output for the designed task planning instruction prompt should be in the following HTML format only:\n",
        "HTML format:\n",
        "<html>\n",
        "<body>\n",
        "    <!-- Section for the Task Overview -->\n",
        "    <section class=\"task-overview\">\n",
        "        <h1>Task Overview and Objective</h1>\n",
        "        <p>Provide a detailed description, overview, and objective of the task here.</p>\n",
        "    </section>\n",
        "\n",
        "    <!-- Section for Step-by-Step Instructions -->\n",
        "    <section class=\"instructions\">\n",
        "        <h2>Pay attention to below Step-by-Step guidelines</h2>\n",
        "        <ol>\n",
        "            <li>Step 1: Description of the first step.</li>\n",
        "            <li>Step 2: Description of the second step.</li>\n",
        "            <li>Step 3: Description of the third step.</li>\n",
        "            <!-- Add more steps as needed -->\n",
        "        </ol>\n",
        "    </section>\n",
        "\n",
        "    <!-- Section for Input Specification -->\n",
        "    <section class=\"input-variables\">\n",
        "        <h2>Task inputs</h2>\n",
        "        <p>provide the input placeholder schema in json for each variables</p> # eg if input is x then specify it in curly brackets\n",
        "\n",
        "    </section>\n",
        "\n",
        "    <!-- Section for Output Format Specification -->\n",
        "    <section class=\"output-format\">\n",
        "        <h2>Output format Details</h2>\n",
        "        <p>Specify how the outcome of the task needs to be</p>\n",
        "        <p>Provide the output schema such that the output of the task is preserved. Also, specify that the outcome of the task should only be the desired output.</p>\n",
        "    </section>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "prompt_gen_hum=\"\"\"Here is the task description: {desc}, these are the input variables {variables} for the task. Desired output format: {output_format}\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "jItZGYvu-Ic2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fill the below feilds according to your task , desired output format and llm choice"
      ],
      "metadata": {
        "id": "7mJ1AcyUKDYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of the prompt, could be anything.\n",
        "prompt_name = 'rag_prompt'\n",
        "\n",
        "# A brief description of the task provided by the user, outlining what the model needs to accomplish.\n",
        "task_desc_user = 'the task is to answer the query based on the given background context'\n",
        "\n",
        "# Specifies the expected output format of the task.\n",
        "# incase specifying a Json, its good to mention what each key of json will be having\n",
        "exp_output_format = 'The output of the task needs to be in json with answer key and reasoning key containing the reasoning used for answering the question'\n",
        "\n",
        "# List of variables that will be input into the LLM. Typically includes the main elements necessary for the model to perform the task.\n",
        "task_input_variables = ['question', 'context']\n",
        "\n",
        "# Specifies the provider of the LLM to be used for generating prompts. Options could be 'openai', 'Anthropic', etc.\n",
        "llm_provider = 'azure_openai'  # 'Anthropic' could be another option\n",
        "\n",
        "# Function to retrieve the specified LLM from the provider. This encapsulates the logic to interact with the chosen LLM's API.\n",
        "llm = get_llm(llm_choice=llm_provider)\n"
      ],
      "metadata": {
        "id": "xjPpbFHm-oz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intiating the langchain chains\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "82yhRFHDLKqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This chain will be used to create a detail task description by collecting user feedback."
      ],
      "metadata": {
        "id": "iO2NDk-7MIxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info_collector=runnable(llm=llm, sys_temp=info_gather_temp_sys, human_temp=info_gather_temp_human )"
      ],
      "metadata": {
        "id": "ejvWjecDLPuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chain will be used to generate prompt using the information supplied to it by the previous chain."
      ],
      "metadata": {
        "id": "1fCJzb2ZMFhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_gen_chain=runnable(llm=llm, sys_temp=prompt_gen_sys, human_temp=prompt_gen_hum)\n"
      ],
      "metadata": {
        "id": "otKuF6cX-qdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once run the below cell, llm wil generate a set of questions and ask for an an answer one at a time"
      ],
      "metadata": {
        "id": "EPoUn_71McV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_desc_w_hum_feedback=interact_and_generate_description(info_collector, brief_task_desc=task_desc_user)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW824Q2j-sZJ",
        "outputId": "0dabb767-adbf-472b-a9b2-7b5f6d3f81fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: What is the nature of the query that needs to be answered?\n",
            "Your answer: n\n",
            "Question 2: Can you provide the specific background context that should be used to answer the query?\n",
            "Your answer: d\n",
            "Question 3: Who is the intended audience for the response to this query?\n",
            "Your answer: \n",
            "Question 4: What is the preferred format for the response (e.g., written report, verbal explanation, presentation)?\n",
            "Your answer: h\n",
            "Question 5: Are there any specific points or aspects that must be included or emphasized in the response?\n",
            "Your answer:  must be included or emphasized in the response\n",
            "#########################\n",
            "\n",
            "\n",
            "\n",
            "##Got the user response now procedding to generate the task description so that prompt planner can plan the excecution of task##\n",
            "\n",
            "\n",
            "\n",
            "#########################\n",
            "Question 1: What is the specific query or topic that needs to be addressed in the answer?\n",
            "Your answer: its is realted to finance \n",
            "Question 2: What is the detailed background context that should be considered when formulating the answer?\n",
            "Your answer: \n",
            "Question 3: Who is the target audience for the answer, and what is their level of expertise or knowledge on the topic?\n",
            "Your answer: general public\n",
            "Question 4: What is the preferred format for delivering the answer (e.g., written report, verbal explanation, presentation)?\n",
            "Your answer: json\n",
            "Question 5: Are there any specific points, data, or arguments that must be included or emphasized in the answer?\n",
            "Your answer: nope\n",
            "#########################\n",
            "\n",
            "\n",
            "\n",
            "##Got the user response now procedding to generate the task description so that prompt planner can plan the excecution of task##\n",
            "\n",
            "\n",
            "\n",
            "#########################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Once the previous cell is executed and answers are provide to llm. Run the below cell to generate the prompt"
      ],
      "metadata": {
        "id": "J1yy8vFSNmmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_gen_response=prompt_gen_chain.invoke({'desc':gen_desc_w_hum_feedback, 'variables':str(task_input_variables),'output_format':exp_output_format })"
      ],
      "metadata": {
        "id": "L2hIkTxX_YRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the below cell to prase and save the prompt"
      ],
      "metadata": {
        "id": "kCOKCWWYN6tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_prompt=parse_and_save_html(prompt_gen_response, file_name=prompt_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bZDQ24RAmAo",
        "outputId": "8e9c771d-7fdc-4992-d3fd-1aa826d632c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Generated prompt is saved at /content/rag_prompt.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEqvsK7KRMjs",
        "outputId": "612db036-6dbc-44d8-b497-8db9d3549035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Task Overview and Objective\n",
            "The task is to create a finance-related answer that is understandable by the general public. The answer should be clear and concise, avoiding the use of jargon or complex concepts that require specific background knowledge. The objective is to provide an informative response that is accessible to individuals with varying levels of expertise in finance.\n",
            "\n",
            "### Pay attention to below Step-by-Step guidelines\n",
            "Step 1: Read the provided question carefully to understand what financial information or explanation is being sought.\n",
            "Step 2: Consider the context in which the question is asked to tailor your answer appropriately.\n",
            "Step 3: Research the topic if necessary to ensure accuracy in your response.\n",
            "Step 4: Draft a clear and concise answer, avoiding the use of technical terms or complex financial concepts that may not be widely understood.\n",
            "Step 5: Provide reasoning for your answer, explaining the thought process or the basis for the information provided in a way that is easy to follow.\n",
            "Step 6: Review your answer and reasoning to ensure they are informative, accurate, and accessible to a general audience.\n",
            "Step 7: Format your answer and reasoning into a JSON structure with the keys 'answer' and 'reasoning'.\n",
            "\n",
            "### Task inputs\n",
            "{\"question\": \"The question to be answered.\", \"context\": \"Any additional information that provides context to the question.\"}\n",
            "\n",
            "### Output format Details\n",
            "The output of the task needs to be in JSON format with two keys: 'answer' and 'reasoning'. The 'answer' key should contain the direct response to the question, while the 'reasoning' key should include the explanation or rationale behind the answer provided.\n",
            "{{\"answer\": \"Your finance-related answer here.\", \"reasoning\": \"The reasoning or explanation supporting the answer provided.\"}}\n",
            "Ensure that the outcome of the task is only the desired output in the specified JSON format.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}